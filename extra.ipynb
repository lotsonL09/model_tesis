{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "d1:  tensor(0.8660)\n",
      "d2:  tensor(2.3452)\n",
      "----------------------------------------------------\n",
      "d1:  tensor(0.1732)\n",
      "d2:  tensor(6.2185)\n",
      "----------------------------------------------------\n",
      "d1:  tensor(0.3464)\n",
      "d2:  tensor(5.2612)\n",
      "----------------------------------------------------\n",
      "d1:  tensor(0.5196)\n",
      "d2:  tensor(10.2191)\n",
      "VAL metric : 1.0000 | FAR metric : 0.0000\n"
     ]
    }
   ],
   "source": [
    "anchor = torch.tensor([[1.0, 2.0, 3.0],[1.2, 2.2, 3.2],[1.1, 2.1, 3.1],[1.6, 2.6, 3.6]])\n",
    "positive = torch.tensor([[1.5, 2.5, 3.5],[1.3, 2.3, 3.3],[1.3, 2.3, 3.3],[1.3, 2.3, 3.3]])\n",
    "negative=torch.tensor([[2.0, 3.5, 4.5],[2.5, 6.5, 7.5],[5.5, 4.5, 1.5],[7.5, 8.5, 9.5]])\n",
    "\n",
    "treshold=1.0\n",
    "\n",
    "count_TA=0\n",
    "\n",
    "count_FA=0\n",
    "\n",
    "count_P=anchor.size(0)\n",
    "\n",
    "for a,p,n in zip(anchor,positive,negative):\n",
    "\n",
    "    d1 = torch.norm(a - p, p=2) # pd +\n",
    "    d2 = torch.norm(a - n, p=2) # nd -\n",
    "\n",
    "    print('----------------------------------------------------')\n",
    "    print('d1: ',d1)\n",
    "    print('d2: ',d2)\n",
    "\n",
    "    if d1 < treshold:\n",
    "        count_TA+=1\n",
    "\n",
    "    if d2 < treshold:\n",
    "        count_FA+=1\n",
    "\n",
    "val=count_TA/count_P\n",
    "\n",
    "far=count_FA/count_P\n",
    "\n",
    "print(f\"VAL metric : {val:.4f} | FAR metric : {far:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_far_metrics(anchors,positives,negatives,treshold=1.1):\n",
    "\n",
    "    count_FA=0\n",
    "    count_TA=0\n",
    "    \n",
    "    count_P=anchors.size(0)\n",
    "\n",
    "    for a,p,n in zip(anchors,positives,negatives):\n",
    "        d1 = torch.norm(a - p, p=2) \n",
    "        d2 = torch.norm(a - n, p=2) \n",
    "        if d1 < treshold:\n",
    "            count_TA+=1\n",
    "\n",
    "        if d2 < treshold:\n",
    "            count_FA+=1\n",
    "\n",
    "    val=count_TA/count_P\n",
    "\n",
    "    far=count_FA/count_P\n",
    "    return val,far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metric,far_metric=get_val_far_metrics(anchors=anchor,\n",
    "                                        positives=positive,\n",
    "                                        negatives=negative)\n",
    "\n",
    "val_metric,far_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.5000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_a=[1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "tensor=torch.Tensor(list_a)\n",
    "\n",
    "torch.mean(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Transforming and augmenting images](https://pytorch.org/vision/stable/transforms.html)\n",
    "\n",
    "[Ilustration of transforms](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('lil_data/a_rostro_248.jpg')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root_data_path=Path('lil_data')\n",
    "\n",
    "image_path=root_data_path / 'a_rostro_248.jpg'\n",
    "\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "from PIL import Image\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "orig_img = Image.open(image_path)\n",
    "\n",
    "## To transform the data\n",
    "\n",
    "augmenter = v2.RandAugment() # random\n",
    "jitter = v2.ColorJitter(brightness=.1,hue=.1) # changes color\n",
    "sharpness_adjuster = v2.RandomAdjustSharpness(sharpness_factor=2)\n",
    "autocontraster = v2.RandomAutocontrast()\n",
    "rotater = v2.RandomRotation(degrees=(0,180))\n",
    "\n",
    "\n",
    "## Changing the data\n",
    "\n",
    "# imgs = [augmenter(orig_img) for _ in range(8)]\n",
    "# imgs = [jitter(orig_img) for _ in range(8)]\n",
    "# imgs = [sharpness_adjuster(orig_img) for _ in range(8)]\n",
    "# imgs = [autocontraster(orig_img) for _ in range(8)]\n",
    "imgs = [rotater(orig_img) for _ in range(4)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_images_path = image_path.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4 augmented images to lil_data\n"
     ]
    }
   ],
   "source": [
    "for i, img in enumerate(imgs):\n",
    "    img_filename = augmented_images_path / f'augmented_image_{i+10}.jpg'\n",
    "    img.save(img_filename)\n",
    "\n",
    "print(f\"Saved {len(imgs)} augmented images to {augmented_images_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
